{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "EQkhe2_L4zUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlzznLGeqGGR"
      },
      "source": [
        "Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "model_checkpoint=\"Helsinki-NLP/opus-mt-en-fr\"\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "raw_data=load_dataset('kde4',lang1='en',lang2='fr')\n",
        "split_dataset=raw_data['train'].train_test_split(train_size=0.8,seed=14)\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def preprocess(examples):\n",
        "  en_inputs=[ex['en'] for ex in examples['translation']]\n",
        "  fr_targets=[ex['fr'] for ex in examples['translation']]\n",
        "  model_inputs=tokenizer(en_inputs,text_target=fr_targets,max_length=64,padding='max_length',truncation=True)\n",
        "\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_datasets = split_dataset.map(preprocess,batched=True,remove_columns=split_dataset[\"train\"].column_names)\n",
        "\n",
        "train_data=tokenized_datasets['train']\n",
        "test_data=tokenized_datasets['test']\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=data_collator, num_workers=2)\n",
        "test_loader= DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=data_collator, num_workers=2)"
      ],
      "metadata": {
        "id": "IlhP0Ir645of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niVf3bmhqYZT"
      },
      "source": [
        "Use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke3-Tf-ozRi_",
        "outputId": "f32dfa6d-e53c-408c-c105-6576702c2a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.is_available()\n",
        "print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1QxOZQsMb5"
      },
      "source": [
        "Define the components of the model(positional encoding, encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTLnYvsb8_zf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(1000) / d_model))\n",
        "\n",
        "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = self.pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(device)\n",
        "\n",
        "class transformer_model(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_len, nhead, feedforward_dim, dropout, num_layers):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(max_len, d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=feedforward_dim, dropout=dropout, batch_first=True)\n",
        "        self.encoder_stack = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=feedforward_dim, dropout=dropout, batch_first=True)\n",
        "        self.decoder_stack = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
        "        self.linear_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        input_embeddings = self.embeddings(x)\n",
        "        input_embeddings = self.positional_encoding(input_embeddings)\n",
        "        tgt_embeddings = self.embeddings(tgt)\n",
        "        tgt_embeddings = self.positional_encoding(tgt_embeddings)\n",
        "\n",
        "        encoder_output = self.encoder_stack(input_embeddings, src_key_padding_mask=src_key_padding_mask)\n",
        "        decoder_output = self.decoder_stack(\n",
        "            tgt_embeddings, encoder_output, tgt_mask=tgt_mask, memory_mask=src_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        output = self.linear_layer(decoder_output)\n",
        "        return output\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFZry3pJsciV"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model parameters\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "d_model = 512\n",
        "max_len = 64\n",
        "nhead = 8\n",
        "feedforward_dim = 2048\n",
        "dropout = 0.1\n",
        "num_layers = 12\n",
        "num_epochs = 3\n",
        "\n",
        "# initialize the model\n",
        "model = transformer_model(vocab_size, d_model, max_len, nhead, feedforward_dim, dropout, num_layers)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize lists to store loss and accuracy for each epoch\n",
        "loss_values = []\n",
        "accuracy_values = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        target_inputs = labels[:, :-1]\n",
        "        target_labels = labels[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        tgt_mask = generate_square_subsequent_mask(target_inputs.size(1)).to(device)\n",
        "        src_key_padding_mask = (input_ids == tokenizer.pad_token_id).to(device)\n",
        "        tgt_key_padding_mask = (target_inputs == tokenizer.pad_token_id).to(device)\n",
        "\n",
        "        outputs = model(input_ids, target_inputs, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        logits = outputs.view(-1, vocab_size)\n",
        "\n",
        "        target_labels_mask = target_labels != tokenizer.pad_token_id\n",
        "        loss = loss_fn(logits[target_labels_mask], target_labels[target_labels_mask])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        correct_predictions += (preds[target_labels_mask] == target_labels[target_labels_mask]).sum().item()\n",
        "        total_tokens += target_labels_mask.sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "    # Store values for plotting\n",
        "    loss_values.append(avg_loss)\n",
        "    accuracy_values.append(accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "-7r4Pfk75J_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33tCgt3es4r3"
      },
      "source": [
        "Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QccVb_1lrThe"
      },
      "outputs": [],
      "source": [
        "# Plotting loss and accuracy over epochs\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), loss_values, marker='o', color='blue')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), accuracy_values, marker='o', color='green')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqgl0zV8s_Ww"
      },
      "source": [
        "fucntion for translating a sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the input sentence\n",
        "        input_ids = tokenizer(sentence, return_tensors=\"pt\", padding='max_length', max_length=64, truncation=True).input_ids.to(device)\n",
        "        tgt_input = torch.zeros((1, 64), dtype=torch.long).to(device)\n",
        "\n",
        "        # Start decoding with the <sos> token\n",
        "        tgt_input[0, 0] = tokenizer.convert_tokens_to_ids(\"<sos>\")\n",
        "        for i in range(1, 64):\n",
        "            outputs = model(input_ids, tgt_input[:, :i])\n",
        "            next_token_logits = outputs[0, i - 1, :]  # Get the last token logits\n",
        "            next_token = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)  # Get the predicted token\n",
        "            tgt_input[0, i] = next_token  # Add predicted token to target input\n",
        "\n",
        "            if next_token.item() == tokenizer.convert_tokens_to_ids(\"<eos>\"):  # if <eos> token is predicted then stop\n",
        "                break\n",
        "\n",
        "        # Decode  result\n",
        "        translated_sentence = tokenizer.decode(tgt_input[0], skip_special_tokens=True)\n",
        "        return translated_sentence\n"
      ],
      "metadata": {
        "id": "Yiy_FXrv5SIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EinmtxnltVyJ"
      },
      "source": [
        "Calculate the BLUE score of the translated sentense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQZmfzSQrxBf"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def calculate_bleu(reference, translated):\n",
        "    reference_tokens = [word_tokenize(reference)]\n",
        "    translated_tokens = word_tokenize(translated)\n",
        "\n",
        "\n",
        "    bleu_score = sentence_bleu(reference_tokens, translated_tokens)\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "english_sentence = \"hello everyone how are you all?\"\n",
        "translated_sentence = translate(english_sentence)\n",
        "reference_sentence = \"bonjour à tous, comment allez-vous?\"\n",
        "\n",
        "\n",
        "bleu_score = calculate_bleu(reference_sentence, translated_sentence)\n",
        "\n",
        "print(f\"Translated: {translated_sentence}\")\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}